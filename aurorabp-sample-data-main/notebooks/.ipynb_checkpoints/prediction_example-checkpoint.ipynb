{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example notebook: BP Prediction\n",
    "This example notebook uses both the `features.tsv` and `participants.tsv` files in the `sample` data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "import os.path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of 'sample' data directory.\n",
    "DATA_DIR=os.path.join('..', 'sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load as dataframes, and join features/participants files on pid field.\n",
    "ppt_df = pd.read_csv(os.path.join(DATA_DIR, 'participants.tsv'), delimiter='\\t')\n",
    "feat_df = pd.read_csv(os.path.join(DATA_DIR, 'features.tsv'), delimiter='\\t')\n",
    "comb_df = ppt_df.merge(feat_df, how='left', left_on='pid', right_on='pid')\n",
    "\n",
    "# View key aspects of combined dataframe: participant id (pid), study phase, and measurement within phase.\n",
    "comb_df[['pid', 'phase', 'measurement']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show alphabetized list of features (see paper for details).\n",
    "feature_list = list(comb_df.columns)\n",
    "feature_list.sort()\n",
    "print(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up features.\n",
    "indep_features = ['baseline_sbp', 'age', 'weight', 'height', 'delta_hr_ekg', 'delta_rpat_pressure']\n",
    "target_feature = 'delta_sbp' # For an easier target, try 'delta_hr_pressure'.\n",
    "\n",
    "# Subset dataframe to contain only the ambulatory measurements by restricting 'phase'.\n",
    "ambulatory_df = comb_df.loc[comb_df['phase'] == 'ambulatory']\n",
    "cv_df = ambulatory_df.dropna(how='any', subset=indep_features+[target_feature])\n",
    "\n",
    "# Compute unique set of participants.\n",
    "ppts = set(cv_df['pid'])\n",
    "print(f'{len(ppts)} participants, {cv_df.shape[0]} total rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation loop over folds per participant. It is essential to stratify by participant vs. by row, \n",
    "# i.e., the training data for a given participant must not contain samples from that participant, since in \n",
    "# realistic problem settings we will not have access to other ambulatory values for that participant.\n",
    "# \n",
    "# The fold strategy below can also be achieved with sklearn's model_selection.LeaveOneGroupOut, but we construct \n",
    "# the groups manually in order to make the stratification clear.\n",
    "test_df_list = []\n",
    "# Choose a model type.\n",
    "model = linear_model.Ridge(alpha=0.001)\n",
    "# Iterate over participants.\n",
    "for ppt in ppts:\n",
    "    # Training subset.\n",
    "    fold_train_df = cv_df.loc[cv_df['pid'] != ppt]\n",
    "    # Create deep copy for test subset, since we'll be adding a column.\n",
    "    fold_test_df = pd.DataFrame(cv_df.loc[cv_df['pid'] == ppt])\n",
    "    # Fit model based on training subset.\n",
    "    model.fit(fold_train_df[indep_features], fold_train_df[target_feature])\n",
    "    # Add 'model_prediction' column to fold_test_df, containing predictions on test subset.\n",
    "    fold_test_df['model_prediction'] = model.predict(fold_test_df[indep_features])\n",
    "    # Append augmented fold_test_df to list for later assembly.\n",
    "    test_df_list.append(fold_test_df)\n",
    "# Assemble all fold_test_dfs into new dataframe\n",
    "predictions_df = pd.concat(test_df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results vs. ground truth.\n",
    "plt.plot(predictions_df[target_feature], predictions_df['model_prediction'], 'b.')\n",
    "plt.plot([-60,40], [-60,40], 'r-')\n",
    "plt.title('Model Prediction vs. Ground Truth\\nwith Proper Stratification\\n(leave one participant out)')\n",
    "plt.xlabel(target_feature)\n",
    "plt.ylabel('Model Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How would it look if we naively used leave-one-(row)-out cross-validation? We expect this will provide us\n",
    "# with unrealistically optimistic results, as explained above, and shown below.\n",
    "demo_df = pd.DataFrame(cv_df)\n",
    "demo_df['false_prediction'] = model_selection.cross_val_predict(model, demo_df[indep_features], demo_df[target_feature], \n",
    "                                                                cv=model_selection.LeaveOneOut())\n",
    "\n",
    "# Plot results vs. ground truth.\n",
    "plt.plot(demo_df[target_feature], demo_df['false_prediction'], 'b.')\n",
    "plt.plot([-60,40], [-60,40], 'r-')\n",
    "plt.title('Model Prediction vs. Ground Truth\\nwithout Proper Stratification\\n(leave one row out)')\n",
    "plt.xlabel(target_feature)\n",
    "plt.ylabel('False Prediction')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b18b75db36c2d0d85abbd33565ed2654ca27e8bd1e1e0321301bf8151a8b2927"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
